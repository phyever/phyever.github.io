<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>

    <!--Description-->

    

    
        <meta name="description" content="Authors: Chen Wei*, Chi Zhang*, Jiachen Zou, Haotian Deng, Dietmar Heinke, Quanying Liu*Chen Wei and Chi Zhang contributed equally to this work.Date: "/>
    

    <!--Author-->
    
        <meta name="author" content="Jiachen Zou"/>
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Modulating Individual Human Percepts"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="Authors: Chen Wei*, Chi Zhang*, Jiachen Zou, Haotian Deng, Dietmar Heinke, Quanying Liu*Chen Wei and Chi Zhang contributed equally to this work.Date: "/>
    

    <!--Open Graph Site Name-->
        <meta property="og:site_name" content="Hi, this is Jiachen"/>

    <!--Type page-->
    
        <meta property="og:type" content="article"/>
    

    <!--Page Cover-->
    
    
        <meta property="og:image" content="http://phyever.github.ioimg/home-bg.jpg"/>
    

        <meta name="twitter:card" content="summary_large_image"/>

    

    
        <meta name="twitter:image" content="http://phyever.github.ioimg/home-bg.jpg"/>
    

    <!-- Title -->
    
    <title>Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Modulating Individual Human Percepts - Hi, this is Jiachen</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/>
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css"/>
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css"/>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet"/>

    <!-- Google Analytics -->
    


    <!-- favicon -->
    

<meta name="generator" content="Hexo 6.3.0"></head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Jiachen Zou's website</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/projects">
                            
                                Projects
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/experience">
                            
                                Experience
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/contact">
                            
                                Contact
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('/img/home-bg.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Modulating Individual Human Percepts</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                        
                            2024-10-02
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p>Authors: Chen Wei*, Chi Zhang*, <strong>Jiachen Zou</strong>, Haotian Deng, Dietmar Heinke, Quanying Liu<br><small>*Chen Wei and Chi Zhang contributed equally to this work.</small><br>Date: October 2, 2024<br>Supervisor: Quanying Liu, Dietmar Heinke</p>
<p><img src="/img/var_fig1.jpg" alt="varMNIST Motivation"></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>Motivation:</strong><br>Human perception varies with ambiguous stimuli, and ANNs struggle to capture this variability. Understanding it is key to predicting decision-making in uncertain situations. This study explores whether generating images based on ANN perceptual boundaries can reveal differences in human perception.</p>
<p><strong>Method:</strong><br>We developed a model that samples ANN boundaries to create images that induce varied human responses. These include stimuli that highlight perceptual differences and adversarial perturbations that subtly influence decisions. We tested these in a large-scale experiment, creating the varMNIST dataset.</p>
<p><strong>Contribution:</strong>  </p>
<ul>
<li>Created a model generating images that provoke diverse human perceptions, forming the varMNIST dataset.  </li>
<li>Improved predictions of human decision-making by aligning ANN variability with human data.  </li>
<li>Uncovered individual differences in perception, supporting personalized AI development.</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><strong>Perceptual Boundaries of ANNs:</strong><br>We sampled images along ANN decision boundaries to induce high perceptual variability. Using two methods:</p>
<ul>
<li><em>Uncertainty Guidance</em>: Maximizes entropy in an ANN’s classification to generate ambiguous images near its decision boundary.</li>
<li><em>Controversial Guidance</em>: Maximizes the KL divergence between two ANN models, producing images that highlight differences in their classifications.</li>
</ul>
<p><strong>Diffusion Model Regularization:</strong><br>To ensure natural-looking images, we used a classifier-free diffusion model that introduces prior information from the MNIST dataset. A reference-constrained method, guided by MSE loss, prevents mode collapse and ensures diverse, recognizable images.</p>
<p><strong>Human Experiments &amp; Dataset Construction:</strong><br>We filtered generated images based on entropy and KL divergence, then tested them in a digit recognition task with human participants. Based on response times and entropy thresholds, we finalized 4,741 images that form the varMNIST dataset.</p>
<figure>
  <img src="/img/var_fig2.jpg" alt="framework">
  <figcaption style="text-align: center; color: #004400; font-weight: bold;">Figure 1: The framework of constructing dataset.</figcaption>
</figure>

<!-- This adds space between the figures -->
<div style="margin-top: 30px;"></div>

<h1 id="Human-Experiments-and-Behavioral-Analysis"><a href="#Human-Experiments-and-Behavioral-Analysis" class="headerlink" title="Human Experiments and Behavioral Analysis"></a>Human Experiments and Behavioral Analysis</h1><!-- This adds space between the figures -->
<div style="margin-top: 10px;"></div>

<p><strong>Entropy Distribution and Image Variability:</strong></p>
<ul>
<li>We used images generated by uncertainty and controversial guidance methods in digit recognition tasks. The example images in Fig.3 indicates the ability of generated images to evoke varying perceptual variability, making entropy a useful metric for image filtering.</li>
</ul>
<figure>
  <img src="/img/var_fig3.png" alt="framework">
  <figcaption style="text-align: center; color: #004400; font-weight: bold;">Figure 2: Entropy distribution of digit recognition by human subjects.</figcaption>
</figure>

<!-- This adds space between the figures -->
<div style="margin-top: 30px;"></div>

<p><strong>Human vs. Model Entropy Comparison:</strong></p>
<ul>
<li>Human response entropy correlated positively with model-predicted entropy, though the model’s predictions initially clustered near zero. After fine-tuning with human behavioral data, the correlation improved significantly, aligning model predictions more closely with human behavior.</li>
</ul>
<p><strong>Response Time (RT) Analysis:</strong>  </p>
<ul>
<li>A positive correlation was found between entropy and RT (Pearson’s correlation&#x3D;1.17e-17), suggesting that higher perceptual variability leads to longer decision times. This makes RT a valuable supplementary measure when filtering images for perceptual variability.</li>
</ul>
<figure>
  <img src="/img/var_fig4.jpg" alt="framework">
  <figcaption style="text-align: center; color: #004400; font-weight: bold;">Figure 3: Quantitative analysis of behavioral measurements. (a) Positive correlation between behavior-calculated and model-predicted entropy. (b) Improvement in the correlation between behavior-calculated and model-predicted entropy after alignment. (c) Distribution of RT. (d) Positive correlation between RT and entropy.</figcaption>
</figure>

<!-- This adds space between the figures -->
<div style="margin-top: 30px;"></div>

<h1 id="Variability-alignment-between-humans-and-networks"><a href="#Variability-alignment-between-humans-and-networks" class="headerlink" title="Variability alignment between humans and networks"></a>Variability alignment between humans and networks</h1><p><strong>Fine-Tuning for Perceptual Alignment:</strong></p>
<ul>
<li>ANN models had low accuracy (0.2802) on varMNIST, revealing a gap in perceptual variability between humans and networks. After:<br><em>Population-Level Fine-Tuning</em> with data from all participants, accuracy improved to 0.5569.<br><em>Subject-Level Fine-Tuning</em> with individual data, accuracy increased to 0.6230, highlighting individual perceptual differences.</li>
</ul>
<figure>
  <img src="/img/var_fig5.jpg" alt="framework">
  <figcaption style="text-align: center; color: #004400; font-weight: bold;">Figure 4: Alignment of model and human behavior on the generated dataset. The x-axis labels are in the format "Training dataset (Test Dataset)", indicating that the model was trained on the "Training dataset" and the accuracy was calculated on the "Test dataset". </figcaption>
</figure>

<!-- This adds space between the figures -->
<div style="margin-top: 30px;"></div>

<p><strong>Subject Clustering Analysis:</strong></p>
<ul>
<li>Participants were grouped into eight clusters based on behavior similarity. Models predicted in-cluster behavior more accurately than out-cluster, confirming significant perceptual differences across participants.</li>
</ul>
<figure>
  <img src="/img/var_fig6.png" alt="framework">
  <figcaption style="text-align: center; color: #004400; font-weight: bold;">Figure 5: Subject clustering analysis. (a) Subject similarity matrix and clustering results. (b) Performance of the subject-finetuned model in predicting data from different groups.</figcaption>
</figure>

<!-- This adds space between the figures -->
<div style="margin-top: 30px;"></div>

<h1 id="Controversial-stimuli-between-subjects-unveil-individual-differences-in-perceptual-variability"><a href="#Controversial-stimuli-between-subjects-unveil-individual-differences-in-perceptual-variability" class="headerlink" title="Controversial stimuli between subjects unveil individual differences in perceptual variability"></a>Controversial stimuli between subjects unveil individual differences in perceptual variability</h1><p><strong>Generating Controversial Stimuli:</strong></p>
<ul>
<li>Using fine-tuned models, we generated controversial stimuli to explore individual perceptual differences. We compared:<br><em>Subject-Subject Testing</em>: Adversarial classifiers created stimuli that caused two individuals to make different decisions, highlighting their perceptual differences.<br><em>Subject-Group Testing</em>: Stimuli were generated between an individual’s fine-tuned model and a group model, revealing differences between the individual and the collective.</li>
</ul>
<p><strong>Disagreement Patterns:</strong> </p>
<ul>
<li>We identified typical patterns in controversial stimuli, called <em>disagreement patterns</em>. For instance, some subjects consistently disagreed on digits like 3 and 7, or 3 and 8. These patterns show that even in seemingly objective tasks like digit recognition, individual differences and preferences exist.</li>
</ul>
<figure>
  <img src="/img/var_fig7.jpg" alt="framework">
  <figcaption style="text-align: center; color: #004400; font-weight: bold;">Figure 6: Overview of patterns among different models in digit classification. (a) Example of disagreement patterns between different models. (b) Distribution of KL divergences between MNIST, varMNIST, and controversial samples. (c) Conceptual diagram of disagreement patterns.</figcaption>
</figure>

<!-- This adds space between the figures -->
<div style="margin-top: 30px;"></div>

<h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p><strong>Key Findings:</strong>  </p>
<ul>
<li>We generated stimuli from ANN perceptual boundaries, effectively evoking diverse human perceptual experiences.  </li>
<li>Controversial stimuli revealed individual differences in human perception, advancing understanding of variability between humans and AI.</li>
</ul>
<p><strong>Advantages Over Prior Work:</strong>  </p>
<ul>
<li>Unlike previous studies that focused on either humans or ANNs, our method influences both, allowing for a more comprehensive analysis of perceptual boundaries.   </li>
<li>The use of diffusion models increases image naturalness and sampling flexibility, enhancing human-AI perceptual alignment.</li>
</ul>
<p><strong>Broader Impact:</strong>  </p>
<ul>
<li>Our framework enables personalized perceptual modulation, making studies on human perception more efficient.  </li>
<li>Combining controversial and adversarial stimuli with diffusion models significantly improves the generation of impactful stimuli.</li>
</ul>
<p><strong>Limitations and Future Work:</strong>  </p>
<ul>
<li>Expanding the dataset to natural images and diverse participants will better capture human variability.  </li>
<li>Further improvements are needed in AI-human alignment using optimal experimental design.</li>
</ul>
<p>This paper has been submitted to ICLR2025.</p>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    


                    

                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2024 Jiachen Zou<br></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->



</body>

</html>